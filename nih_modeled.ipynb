{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BQ Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"nih_modeled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'probable-pager-266720:nih_modeled'\n",
      "already exists.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US mk --dataset {dataset_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r4c283f39cc328099_0000017074de3801_1 ... (15s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.arm_groups \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-ArmGroups.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r52cf88b36110fa43_0000017074de82a7_1 ... (0s) Current status: DONE   \n",
      "BigQuery error in load operation: Error processing job 'probable-\n",
      "pager-266720:bqjob_r52cf88b36110fa43_0000017074de82a7_1': Provided Schema does\n",
      "not match Table probable-pager-266720:nih_modeled.clinical_results. Cannot add\n",
      "fields (field: description)\n",
      "Failure details:\n",
      "- It looks like you are appending to an existing table with\n",
      "autodetect enabled. Disabling autodetect may resolve this.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.clinical_results \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-ClinicalResults.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r7d5471dbf9a826fe_00000170898563db_1 ... (0s) Current status: DONE   \n",
      "BigQuery error in load operation: Error processing job 'probable-\n",
      "pager-266720:bqjob_r7d5471dbf9a826fe_00000170898563db_1': Provided Schema does\n",
      "not match Table probable-pager-266720:nih_modeled.clinical_studies_main. Field\n",
      "start_date has changed type from DATE to TIMESTAMP\n",
      "Failure details:\n",
      "- It looks like you are appending to an existing table with\n",
      "autodetect enabled. Disabling autodetect may resolve this.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.clinical_studies_main \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-ClinicalStudiesMain.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.collaborators \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-Collaborators.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.contacts \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-Contacts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.interventions \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-Interventions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.locations \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-Locations.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.other_outcomes \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-OtherOutcomes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.primary_outcomes \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-PrimaryOutcomes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.secondary_outcomes \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-SecondaryOutcomes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US load --autodetect --skip_leading_rows=1 \\\n",
    "--source_format=CSV {dataset_id}.responsible_parties \\\n",
    "'gs://clinical_trials_ncl/Clinical Trials/NIHClinicalTrials-ResponsibleParties.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeled Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.eligibility AS \n",
    "SELECT nct_number, eligibility_study_pop, eligibility_sampling_method, eligibility_criteria, eligibility_gender, eligibility_minimum_age, eligibility_maximum_age, eligibility_healthy_volunteers \n",
    "FROM nih_staging.clinical_studies_main AS csm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery \n",
    "\n",
    "CREATE TABLE nih_modeled.contact AS \n",
    "SELECT nct_number, first_name, middle_name, last_name, degrees, phone, phone_ext, email, serialid\n",
    "FROM nih_staging.contacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery \n",
    "\n",
    "CREATE TABLE nih_modeled.clinical_results AS \n",
    "SELECT nct_number, type, title, time_frame, safety_issue,  results_population, serialid\n",
    "FROM nih_staging.clinical_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery \n",
    "\n",
    "CREATE TABLE nih_modeled.arm_group AS \n",
    "SELECT nct_number, arm_group_label, arm_group_type, description, serialid\n",
    "FROM nih_staging.arm_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: e3748fad-86dd-4dc8-b320-79415774cd67\n",
      "Query executing: 0.28s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 409 GET https://bigquery.googleapis.com/bigquery/v2/projects/probable-pager-266720/queries/e3748fad-86dd-4dc8-b320-79415774cd67?maxResults=0&location=US&timeoutMs=400: Already Exists: Table probable-pager-266720:nih_modeled.clinical_studies_main\n",
      "\n",
      "(job ID: e3748fad-86dd-4dc8-b320-79415774cd67)\n",
      "\n",
      "                                                                                                                                                                                                                                                                -----Query Job SQL Follows-----                                                                                                                                                                                                                                                                \n",
      "\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n",
      "   1:CREATE TABLE nih_modeled.clinical_studies_main AS \n",
      "   2:SELECT nct_number, org_study_id, secondary_id, official_title, brief_summary, overall_status, enrollment, enrollment_type, CAST(start_date AS DATE) AS start_date, CAST(completion_date AS DATE) AS competion_date, condition, number_of_arms, number_of_groups, phase, study_type, study_design, CAST(first_received_date AS DATE) AS first_recieved_date, CAST(verification_date AS DATE) AS verification_date, lead_sponsor_agency, lead_sponsor_agency_class, overall_official_last_name AS official_full_name, overall_official_affiliation, serialid\n",
      "   3:FROM nih_staging.clinical_studies_main\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.clinical_studies_main AS \n",
    "SELECT nct_number, org_study_id, secondary_id, official_title, brief_summary, overall_status, enrollment, enrollment_type, CAST(start_date AS DATE) AS start_date, CAST(completion_date AS DATE) AS competion_date, condition, number_of_arms, number_of_groups, phase, study_type, study_design, CAST(first_received_date AS DATE) AS first_recieved_date, CAST(verification_date AS DATE) AS verification_date, lead_sponsor_agency, lead_sponsor_agency_class, overall_official_last_name AS official_full_name, overall_official_affiliation, serialid\n",
    "FROM nih_staging.clinical_studies_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.collaborators AS \n",
    "SELECT nct_number, collaborator_agency, collaborator_agency_class, serialid\n",
    "FROM nih_staging.collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.contacts AS \n",
    "SELECT nct_number, last_name, phone, phone_ext, email, serialid\n",
    "FROM nih_staging.contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.interventions AS \n",
    "SELECT nct_number, intervention_type, intervention_name, arm_group_label, serialid\n",
    "FROM nih_staging.interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.locations AS \n",
    "SELECT nct_number, facility_name, facility_city, facility_state, facility_zip, facility_country, serialid\n",
    "FROM nih_staging.locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.responsible_parties AS \n",
    "SELECT nct_number,name_title, organization, type, investigator_affiliation, investigator_full_name, investigator_title, serialid\n",
    "FROM nih_staging.responsible_parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.primary_outcomes AS \n",
    "SELECT nct_number, measure, time_frame, safety_issue, description, serialid\n",
    "FROM nih_staging.primary_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.secondary_outcomes AS \n",
    "SELECT nct_number, measure, time_frame, safety_issue, description, serialid\n",
    "FROM nih_staging.secondary_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE nih_modeled.other_outcomes AS \n",
    "SELECT nct_number, measure, time_frame, safety_issue, outcome_description as description, serialid\n",
    "FROM nih_staging.other_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arm Groups Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>394356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  394356\n",
       "1  191171"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.arm_groups ag\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.arm_groups ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>arm_group_label</th>\n",
       "      <th>arm_group_type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01234129</td>\n",
       "      <td>zoledronic acid or not zoledronic acid</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT01230008</td>\n",
       "      <td>Radiotherapy in mediastinal lymphoma</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT03024333</td>\n",
       "      <td>Healthy subjects</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00524212</td>\n",
       "      <td>IE confirmed IE rejected</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT03024450</td>\n",
       "      <td>HER2 positive AGC treated with H+CT</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT03024385</td>\n",
       "      <td>Mothers of healthy infants</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT03024294</td>\n",
       "      <td>Patients undergoing VATS lobectomy or segmente...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT03024489</td>\n",
       "      <td>Palbociclib-Cetiximab-IMRT</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT03024437</td>\n",
       "      <td>Phase II - Cohort B</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT03024411</td>\n",
       "      <td>Group 1</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                    arm_group_label  \\\n",
       "0  NCT01234129             zoledronic acid or not zoledronic acid   \n",
       "1  NCT01230008               Radiotherapy in mediastinal lymphoma   \n",
       "2  NCT03024333                                   Healthy subjects   \n",
       "3  NCT00524212                           IE confirmed IE rejected   \n",
       "4  NCT03024450                HER2 positive AGC treated with H+CT   \n",
       "5  NCT03024385                         Mothers of healthy infants   \n",
       "6  NCT03024294  Patients undergoing VATS lobectomy or segmente...   \n",
       "7  NCT03024489                         Palbociclib-Cetiximab-IMRT   \n",
       "8  NCT03024437                                Phase II - Cohort B   \n",
       "9  NCT03024411                                            Group 1   \n",
       "\n",
       "  arm_group_type  count  \n",
       "0           None      2  \n",
       "1           None      2  \n",
       "2           None      2  \n",
       "3           None      2  \n",
       "4           None      2  \n",
       "5           None      2  \n",
       "6          Other      2  \n",
       "7   Experimental      2  \n",
       "8   Experimental      2  \n",
       "9   Experimental      2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, arm_group_label, arm_group_type, count(nct_number) as count\n",
    "from nih_modeled.arm_groups ag\n",
    "group by nct_number, arm_group_label, arm_group_type\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>349288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  349288\n",
       "1   23895"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.clinical_results cr\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.clinical_results cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>time_frame</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00019604</td>\n",
       "      <td>Primary</td>\n",
       "      <td>Response</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00324233</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Subjectively Measured Handling</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00324233</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Subjectively Measured Insertion of the Catheter</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00386776</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Completeness of Patients' Problem Lists</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00386776</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Time Per Visit</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00386776</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Number of Telephone Calls and E-mail Messages ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00386776</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Number of Office Visits by Patients</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00491374</td>\n",
       "      <td>Primary</td>\n",
       "      <td>The Change From Baseline in the Number of Apne...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00521339</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Time to Achieve PASI-75 During Treatment Phase</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00521339</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>Time to Clinically Relevant Response (Time to ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number       type                                              title  \\\n",
       "0  NCT00019604    Primary                                           Response   \n",
       "1  NCT00324233  Secondary                     Subjectively Measured Handling   \n",
       "2  NCT00324233  Secondary    Subjectively Measured Insertion of the Catheter   \n",
       "3  NCT00386776  Secondary            Completeness of Patients' Problem Lists   \n",
       "4  NCT00386776  Secondary                                     Time Per Visit   \n",
       "5  NCT00386776  Secondary  Number of Telephone Calls and E-mail Messages ...   \n",
       "6  NCT00386776  Secondary                Number of Office Visits by Patients   \n",
       "7  NCT00491374    Primary  The Change From Baseline in the Number of Apne...   \n",
       "8  NCT00521339  Secondary     Time to Achieve PASI-75 During Treatment Phase   \n",
       "9  NCT00521339  Secondary  Time to Clinically Relevant Response (Time to ...   \n",
       "\n",
       "  time_frame  count  \n",
       "0       None      2  \n",
       "1       None      2  \n",
       "2       None      2  \n",
       "3       None      2  \n",
       "4       None      2  \n",
       "5       None      2  \n",
       "6       None      2  \n",
       "7       None      2  \n",
       "8       None      2  \n",
       "9       None      2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, type, title, time_frame, count(nct_number) as count\n",
    "from nih_modeled.clinical_results cr\n",
    "group by nct_number, type, title, time_frame\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Studies Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  704169\n",
       "1  234703"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.clinical_studies_main csm\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.clinical_studies_main csm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>org_study_id</th>\n",
       "      <th>secondary_id</th>\n",
       "      <th>official_title</th>\n",
       "      <th>overall_status</th>\n",
       "      <th>enrollment</th>\n",
       "      <th>enrollment_type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00541632</td>\n",
       "      <td>010737</td>\n",
       "      <td>None</td>\n",
       "      <td>BacLite Rapid MRSA Clinical Performance</td>\n",
       "      <td>Terminated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT01650493</td>\n",
       "      <td>CR-11-012</td>\n",
       "      <td>None</td>\n",
       "      <td>In-situ Evaluation of Anti-caries Technology</td>\n",
       "      <td>Completed</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT01064843</td>\n",
       "      <td>3M ESPE CR-09-020</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical Evaluation of the Mandibular Mini-Imp...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01369134</td>\n",
       "      <td>CR-10-014</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical Evaluation of 3M ESPE Adper Easy Bond...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00643461</td>\n",
       "      <td>CR-07-009</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical Evaluation of Three Dental Adhesive S...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01138592</td>\n",
       "      <td>05-003041</td>\n",
       "      <td>None</td>\n",
       "      <td>Pilot Study to Evaluate the Clinical Utility o...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Anticipated</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT01665820</td>\n",
       "      <td>Study EM-05-012530</td>\n",
       "      <td>None</td>\n",
       "      <td>Study EM-05-012530 Benefit of Auscultation Wit...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT01464294</td>\n",
       "      <td>3M ESPE CR-10-019</td>\n",
       "      <td>None</td>\n",
       "      <td>Clinical Evaluation of Chairside CAD/CAM Nano-...</td>\n",
       "      <td>Terminated</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT01500187</td>\n",
       "      <td>CR-11-011</td>\n",
       "      <td>ORTHO2011-01</td>\n",
       "      <td>Fluoride Varnish Application for Treatment of ...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Anticipated</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00731042</td>\n",
       "      <td>I2MS05-010877</td>\n",
       "      <td>None</td>\n",
       "      <td>A Two-week Crossover Evaluation of Two Hand An...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>33.0</td>\n",
       "      <td>Actual</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number        org_study_id  secondary_id  \\\n",
       "0  NCT00541632              010737          None   \n",
       "1  NCT01650493           CR-11-012          None   \n",
       "2  NCT01064843   3M ESPE CR-09-020          None   \n",
       "3  NCT01369134           CR-10-014          None   \n",
       "4  NCT00643461           CR-07-009          None   \n",
       "5  NCT01138592           05-003041          None   \n",
       "6  NCT01665820  Study EM-05-012530          None   \n",
       "7  NCT01464294   3M ESPE CR-10-019          None   \n",
       "8  NCT01500187           CR-11-011  ORTHO2011-01   \n",
       "9  NCT00731042       I2MS05-010877          None   \n",
       "\n",
       "                                      official_title overall_status  \\\n",
       "0            BacLite Rapid MRSA Clinical Performance     Terminated   \n",
       "1       In-situ Evaluation of Anti-caries Technology      Completed   \n",
       "2  Clinical Evaluation of the Mandibular Mini-Imp...      Completed   \n",
       "3  Clinical Evaluation of 3M ESPE Adper Easy Bond...      Completed   \n",
       "4  Clinical Evaluation of Three Dental Adhesive S...      Completed   \n",
       "5  Pilot Study to Evaluate the Clinical Utility o...      Completed   \n",
       "6  Study EM-05-012530 Benefit of Auscultation Wit...      Completed   \n",
       "7  Clinical Evaluation of Chairside CAD/CAM Nano-...     Terminated   \n",
       "8  Fluoride Varnish Application for Treatment of ...      Completed   \n",
       "9  A Two-week Crossover Evaluation of Two Hand An...      Completed   \n",
       "\n",
       "   enrollment enrollment_type  count  \n",
       "0         NaN            None      3  \n",
       "1        29.0          Actual      3  \n",
       "2        80.0          Actual      3  \n",
       "3        40.0          Actual      3  \n",
       "4        46.0          Actual      3  \n",
       "5       150.0     Anticipated      3  \n",
       "6        30.0          Actual      3  \n",
       "7       120.0          Actual      3  \n",
       "8        30.0     Anticipated      3  \n",
       "9        33.0          Actual      3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, org_study_id, secondary_id, official_title, overall_status, enrollment, enrollment_type, count(nct_number) as count\n",
    "from nih_modeled.clinical_studies_main\n",
    "group by nct_number, org_study_id, secondary_id, official_title, overall_status, enrollment, enrollment_type\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  263694\n",
       "1   78684"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.collaborators col\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.collaborators col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>collaborator_agency</th>\n",
       "      <th>collaborator_agency_class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01382446</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00386594</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT02754791</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00408473</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT02220101</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01649596</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00193908</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00175643</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00230373</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT02030002</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number collaborator_agency collaborator_agency_class  count\n",
       "0  NCT01382446                  3M                  Industry      2\n",
       "1  NCT00386594                  3M                  Industry      2\n",
       "2  NCT02754791                  3M                  Industry      2\n",
       "3  NCT00408473                  3M                  Industry      2\n",
       "4  NCT02220101                  3M                  Industry      2\n",
       "5  NCT01649596                  3M                  Industry      2\n",
       "6  NCT00193908                  3M                  Industry      2\n",
       "7  NCT00175643                  3M                  Industry      2\n",
       "8  NCT00230373                  3M                  Industry      2\n",
       "9  NCT02030002                  3M                  Industry      2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, collaborator_agency, collaborator_agency_class, count(nct_number) as count\n",
    "from nih_modeled.collaborators\n",
    "group by nct_number, collaborator_agency, collaborator_agency_class\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contacts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  308354\n",
       "1   37569"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.contacts con\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.contacts con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>last_name</th>\n",
       "      <th>phone</th>\n",
       "      <th>phone_ext</th>\n",
       "      <th>email</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00001154</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00001159</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00001160</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00001162</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00001163</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>(888) NCI-1937</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00001168</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00001174</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00001177</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00001183</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00001184</td>\n",
       "      <td>For more information at the NIH Clinical Cente...</td>\n",
       "      <td>800-411-1222</td>\n",
       "      <td>TTY8664111010</td>\n",
       "      <td>prpl@mail.cc.nih.gov</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                          last_name  \\\n",
       "0  NCT00001154  For more information at the NIH Clinical Cente...   \n",
       "1  NCT00001159  For more information at the NIH Clinical Cente...   \n",
       "2  NCT00001160  For more information at the NIH Clinical Cente...   \n",
       "3  NCT00001162  For more information at the NIH Clinical Cente...   \n",
       "4  NCT00001163  For more information at the NIH Clinical Cente...   \n",
       "5  NCT00001168  For more information at the NIH Clinical Cente...   \n",
       "6  NCT00001174  For more information at the NIH Clinical Cente...   \n",
       "7  NCT00001177  For more information at the NIH Clinical Cente...   \n",
       "8  NCT00001183  For more information at the NIH Clinical Cente...   \n",
       "9  NCT00001184  For more information at the NIH Clinical Cente...   \n",
       "\n",
       "            phone      phone_ext                 email  count  \n",
       "0    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "1    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "2    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "3    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "4  (888) NCI-1937           None                  None      2  \n",
       "5    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "6    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "7    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "8    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  \n",
       "9    800-411-1222  TTY8664111010  prpl@mail.cc.nih.gov      2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, last_name, phone, phone_ext, email, count(nct_number) as count\n",
    "from nih_modeled.contacts\n",
    "group by nct_number, last_name, phone, phone_ext, email\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  704169\n",
       "1  234703"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.eligibility el\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.eligibility el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>eligibility_study_pop</th>\n",
       "      <th>eligibility_sampling_method</th>\n",
       "      <th>eligibility_criteria</th>\n",
       "      <th>eligibility_gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01988025</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00047957</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n        INCLUSION CRITERIA:\\n\\n        Twent...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT02250573</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00245076</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n        DISEASE CHARACTERISTICS:\\n\\n        ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT01168609</td>\n",
       "      <td>\\n        Between December 2007 and March 2009...</td>\n",
       "      <td>Non-Probability Sample</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01323842</td>\n",
       "      <td>\\n        ED patients with suspected renal col...</td>\n",
       "      <td>Non-Probability Sample</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT02549313</td>\n",
       "      <td>\\n        Patient with brain lesion\\n</td>\n",
       "      <td>Non-Probability Sample</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00543998</td>\n",
       "      <td>\\n        primary care clinic,\\n</td>\n",
       "      <td>Non-Probability Sample</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>All</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00721708</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT02698878</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\\n        Inclusion Criteria:\\n\\n          -  ...</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                              eligibility_study_pop  \\\n",
       "0  NCT01988025                                               None   \n",
       "1  NCT00047957                                               None   \n",
       "2  NCT02250573                                               None   \n",
       "3  NCT00245076                                               None   \n",
       "4  NCT01168609  \\n        Between December 2007 and March 2009...   \n",
       "5  NCT01323842  \\n        ED patients with suspected renal col...   \n",
       "6  NCT02549313        \\n        Patient with brain lesion\\n         \n",
       "7  NCT00543998             \\n        primary care clinic,\\n         \n",
       "8  NCT00721708                                               None   \n",
       "9  NCT02698878                                               None   \n",
       "\n",
       "  eligibility_sampling_method  \\\n",
       "0                        None   \n",
       "1                        None   \n",
       "2                        None   \n",
       "3                        None   \n",
       "4      Non-Probability Sample   \n",
       "5      Non-Probability Sample   \n",
       "6      Non-Probability Sample   \n",
       "7      Non-Probability Sample   \n",
       "8                        None   \n",
       "9                        None   \n",
       "\n",
       "                                eligibility_criteria eligibility_gender  count  \n",
       "0                                               None               None      3  \n",
       "1  \\n        INCLUSION CRITERIA:\\n\\n        Twent...                All      3  \n",
       "2  \\n        Inclusion Criteria:\\n\\n          -  ...                All      3  \n",
       "3  \\n        DISEASE CHARACTERISTICS:\\n\\n        ...                All      3  \n",
       "4  \\n        Inclusion Criteria:\\n\\n          -  ...                All      3  \n",
       "5  \\n        Inclusion Criteria:\\n\\n          -  ...                All      3  \n",
       "6  \\n        Inclusion Criteria:\\n\\n          -  ...                All      3  \n",
       "7  \\n        Inclusion Criteria:\\n\\n          -  ...                All      3  \n",
       "8  \\n        Inclusion Criteria:\\n\\n          -  ...             Female      3  \n",
       "9  \\n        Inclusion Criteria:\\n\\n          -  ...               Male      3  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, eligibility_study_pop, eligibility_sampling_method, eligibility_criteria, eligibility_gender, count(nct_number) as count\n",
    "from nih_modeled.eligibility\n",
    "group by nct_number, eligibility_study_pop, eligibility_sampling_method, eligibility_criteria, eligibility_gender\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>816878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  816878\n",
       "1  207014"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.interventions inter\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.interventions inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>intervention_type</th>\n",
       "      <th>intervention_name</th>\n",
       "      <th>arm_group_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01056822</td>\n",
       "      <td>Drug</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT01056822</td>\n",
       "      <td>Drug</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT01525511</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>Group A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01026116</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT01218932</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01199627</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT01416363</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>Firategrast immediate release tablet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00728715</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT01424462</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>Firategrast IR</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT01267071</td>\n",
       "      <td>Drug</td>\n",
       "      <td>A</td>\n",
       "      <td>A, B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number intervention_type intervention_name  \\\n",
       "0  NCT01056822              Drug                 1   \n",
       "1  NCT01056822              Drug                 2   \n",
       "2  NCT01525511              Drug                 A   \n",
       "3  NCT01026116              Drug                 A   \n",
       "4  NCT01218932              Drug                 A   \n",
       "5  NCT01199627              Drug                 A   \n",
       "6  NCT01416363              Drug                 A   \n",
       "7  NCT00728715              Drug                 A   \n",
       "8  NCT01424462              Drug                 A   \n",
       "9  NCT01267071              Drug                 A   \n",
       "\n",
       "                        arm_group_label  count  \n",
       "0                                     1      2  \n",
       "1                                     2      2  \n",
       "2                               Group A      2  \n",
       "3                                  None      2  \n",
       "4                                     A      2  \n",
       "5                                     A      2  \n",
       "6  Firategrast immediate release tablet      2  \n",
       "7                                     A      2  \n",
       "8                        Firategrast IR      2  \n",
       "9                                  A, B      2  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, intervention_type, intervention_name, arm_group_label, count(nct_number) as count\n",
    "from nih_modeled.interventions\n",
    "group by nct_number, intervention_type, intervention_name, arm_group_label\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3514360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count\n",
       "0  3514360\n",
       "1   207903"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.locations loc\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.locations loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>facility_name</th>\n",
       "      <th>facility_city</th>\n",
       "      <th>facility_state</th>\n",
       "      <th>facility_zip</th>\n",
       "      <th>facility_country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT02515357</td>\n",
       "      <td>Harokopio University</td>\n",
       "      <td>Kallithea</td>\n",
       "      <td>Attiki</td>\n",
       "      <td>17671</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00974701</td>\n",
       "      <td>Prince of Wales Hospital, Hong Kong, China</td>\n",
       "      <td>Shating</td>\n",
       "      <td>N.t.</td>\n",
       "      <td>None</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT02095314</td>\n",
       "      <td>Garuda Primary Health Center</td>\n",
       "      <td>Bandung</td>\n",
       "      <td>West Java</td>\n",
       "      <td>None</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00891254</td>\n",
       "      <td>Hospital de La Serena</td>\n",
       "      <td>La Serena</td>\n",
       "      <td>Iv Region</td>\n",
       "      <td>IV REGION</td>\n",
       "      <td>Chile</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00800358</td>\n",
       "      <td>Haemodialysis Unit, Serdang Hospital</td>\n",
       "      <td>Serdang</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>43000</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT02246296</td>\n",
       "      <td>Kilifi County Hospital</td>\n",
       "      <td>Kilifi</td>\n",
       "      <td>Coast</td>\n",
       "      <td>80108</td>\n",
       "      <td>Kenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT02097654</td>\n",
       "      <td>University College Cork</td>\n",
       "      <td>Cork</td>\n",
       "      <td>Munster</td>\n",
       "      <td>2</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT01594931</td>\n",
       "      <td>Bethesday Hospital</td>\n",
       "      <td>Tomohon</td>\n",
       "      <td>North Sulawesi</td>\n",
       "      <td>None</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT01770210</td>\n",
       "      <td>General State Hospital</td>\n",
       "      <td>Polygyros</td>\n",
       "      <td>Chalkidiki</td>\n",
       "      <td>None</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT02036034</td>\n",
       "      <td>University Malaya Medical Center</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Wilayah Persekutuan</td>\n",
       "      <td>59100</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                               facility_name facility_city  \\\n",
       "0  NCT02515357                        Harokopio University     Kallithea   \n",
       "1  NCT00974701  Prince of Wales Hospital, Hong Kong, China       Shating   \n",
       "2  NCT02095314                Garuda Primary Health Center       Bandung   \n",
       "3  NCT00891254                       Hospital de La Serena     La Serena   \n",
       "4  NCT00800358        Haemodialysis Unit, Serdang Hospital       Serdang   \n",
       "5  NCT02246296                      Kilifi County Hospital        Kilifi   \n",
       "6  NCT02097654                     University College Cork          Cork   \n",
       "7  NCT01594931                          Bethesday Hospital       Tomohon   \n",
       "8  NCT01770210                      General State Hospital     Polygyros   \n",
       "9  NCT02036034            University Malaya Medical Center  Kuala Lumpur   \n",
       "\n",
       "        facility_state facility_zip facility_country  count  \n",
       "0               Attiki        17671           Greece      2  \n",
       "1                 N.t.         None        Hong Kong      2  \n",
       "2            West Java         None        Indonesia      2  \n",
       "3            Iv Region    IV REGION            Chile      2  \n",
       "4             Selangor        43000         Malaysia      2  \n",
       "5                Coast        80108            Kenya      2  \n",
       "6              Munster            2          Ireland      2  \n",
       "7       North Sulawesi         None        Indonesia      2  \n",
       "8           Chalkidiki         None           Greece      2  \n",
       "9  Wilayah Persekutuan        59100         Malaysia      2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, facility_name, facility_city, facility_state, facility_zip, facility_country, count(nct_number) as count\n",
    "from nih_modeled.locations\n",
    "group by nct_number, facility_name, facility_city, facility_state, facility_zip, facility_country\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsible Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  429326\n",
       "1  214643"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.responsible_parties rp\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.responsible_parties rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>name_title</th>\n",
       "      <th>organization</th>\n",
       "      <th>type</th>\n",
       "      <th>investigator_affiliation</th>\n",
       "      <th>investigator_full_name</th>\n",
       "      <th>investigator_title</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00633490</td>\n",
       "      <td>Jin Li/Dr</td>\n",
       "      <td>Fudan University cancer hospital</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00115999</td>\n",
       "      <td>Brian Kersten, PhD</td>\n",
       "      <td>Nuvelo, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT01129856</td>\n",
       "      <td>Stephen From / President and Chief Executive O...</td>\n",
       "      <td>Eyegate Pharmaceuticals, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00914368</td>\n",
       "      <td>Lars Wallentin, MD, PhD. Professor Cardiology</td>\n",
       "      <td>UCR, Uppsala University</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00707824</td>\n",
       "      <td>Assistant Professor Orawan Pongraweewan</td>\n",
       "      <td>Mahidol University</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00465907</td>\n",
       "      <td>Dr Alex Chang / Medical Director</td>\n",
       "      <td>Johns Hopkins Singapore International Medical ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00623532</td>\n",
       "      <td>Isabelle Bourdel-Marchasson/ Professor (MD-PhD)</td>\n",
       "      <td>Pole of Gerontology-CHU of Bordeaux/ CNRS 5536...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00983931</td>\n",
       "      <td>Vice President, Branded Products and Medical A...</td>\n",
       "      <td>Mutual Pharmaceutical Company, Inc.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00350454</td>\n",
       "      <td>Prof. A. Schmig</td>\n",
       "      <td>Deutsches Herzzentrum Munich</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT01065688</td>\n",
       "      <td>Second Department of Surgery</td>\n",
       "      <td>Wakayama Medical University</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                         name_title  \\\n",
       "0  NCT00633490                                          Jin Li/Dr   \n",
       "1  NCT00115999                                 Brian Kersten, PhD   \n",
       "2  NCT01129856  Stephen From / President and Chief Executive O...   \n",
       "3  NCT00914368      Lars Wallentin, MD, PhD. Professor Cardiology   \n",
       "4  NCT00707824            Assistant Professor Orawan Pongraweewan   \n",
       "5  NCT00465907                   Dr Alex Chang / Medical Director   \n",
       "6  NCT00623532    Isabelle Bourdel-Marchasson/ Professor (MD-PhD)   \n",
       "7  NCT00983931  Vice President, Branded Products and Medical A...   \n",
       "8  NCT00350454                                   Prof. A. Schmig   \n",
       "9  NCT01065688                       Second Department of Surgery   \n",
       "\n",
       "                                        organization  type  \\\n",
       "0                   Fudan University cancer hospital  None   \n",
       "1                                       Nuvelo, Inc.  None   \n",
       "2                      Eyegate Pharmaceuticals, Inc.  None   \n",
       "3                            UCR, Uppsala University  None   \n",
       "4                                 Mahidol University  None   \n",
       "5  Johns Hopkins Singapore International Medical ...  None   \n",
       "6  Pole of Gerontology-CHU of Bordeaux/ CNRS 5536...  None   \n",
       "7                Mutual Pharmaceutical Company, Inc.  None   \n",
       "8                       Deutsches Herzzentrum Munich  None   \n",
       "9                        Wakayama Medical University  None   \n",
       "\n",
       "  investigator_affiliation investigator_full_name investigator_title  count  \n",
       "0                     None                   None               None      2  \n",
       "1                     None                   None               None      2  \n",
       "2                     None                   None               None      2  \n",
       "3                     None                   None               None      2  \n",
       "4                     None                   None               None      2  \n",
       "5                     None                   None               None      2  \n",
       "6                     None                   None               None      2  \n",
       "7                     None                   None               None      2  \n",
       "8                     None                   None               None      2  \n",
       "9                     None                   None               None      2  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, name_title, organization, type, investigator_affiliation, investigator_full_name, investigator_title, count(nct_number) as count\n",
    "from nih_modeled.responsible_parties\n",
    "group by nct_number, name_title, organization, type, investigator_affiliation, investigator_full_name, investigator_title\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Outcomes Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>672442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  672442\n",
       "1  217421"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.primary_outcomes po\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.primary_outcomes po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>measure</th>\n",
       "      <th>time_frame</th>\n",
       "      <th>safety_issue</th>\n",
       "      <th>description</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00202488</td>\n",
       "      <td>Difference in the presence of genes related to...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00252941</td>\n",
       "      <td>CT at 1 month post-brachytherapy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00000328</td>\n",
       "      <td>Addiction Severity Index (ASI) Composite Score...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00234806</td>\n",
       "      <td>Evaluation of the impact of telehealth on util...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00472355</td>\n",
       "      <td>Effects on parkinsonism measured with finger a...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01122654</td>\n",
       "      <td>Body weight measured on calibrated infant scal...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00015223</td>\n",
       "      <td>Psychosocial counseling sessions</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00000321</td>\n",
       "      <td>AIDS medical care engagement</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00248404</td>\n",
       "      <td>Safety/tolerability of NB1011 infusions</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00166816</td>\n",
       "      <td>Dose-concentration relationship of sirolimus</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                            measure time_frame  \\\n",
       "0  NCT00202488  Difference in the presence of genes related to...       None   \n",
       "1  NCT00252941                   CT at 1 month post-brachytherapy       None   \n",
       "2  NCT00000328  Addiction Severity Index (ASI) Composite Score...       None   \n",
       "3  NCT00234806  Evaluation of the impact of telehealth on util...       None   \n",
       "4  NCT00472355  Effects on parkinsonism measured with finger a...       None   \n",
       "5  NCT01122654  Body weight measured on calibrated infant scal...       None   \n",
       "6  NCT00015223                   Psychosocial counseling sessions       None   \n",
       "7  NCT00000321                       AIDS medical care engagement       None   \n",
       "8  NCT00248404            Safety/tolerability of NB1011 infusions       None   \n",
       "9  NCT00166816       Dose-concentration relationship of sirolimus       None   \n",
       "\n",
       "  safety_issue description  count  \n",
       "0         None        None      2  \n",
       "1         None        None      2  \n",
       "2         None        None      2  \n",
       "3         None        None      2  \n",
       "4         None        None      2  \n",
       "5         None        None      2  \n",
       "6         None        None      2  \n",
       "7         None        None      2  \n",
       "8         None        None      2  \n",
       "9         None        None      2  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, measure, time_frame, safety_issue, description, count(nct_number) as count\n",
    "from nih_modeled.primary_outcomes\n",
    "group by nct_number, measure, time_frame, safety_issue, description\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Outcomes Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1387482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count\n",
       "0  1387482\n",
       "1   167637"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.secondary_outcomes so\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.secondary_outcomes so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>measure</th>\n",
       "      <th>time_frame</th>\n",
       "      <th>safety_issue</th>\n",
       "      <th>description</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT01644097</td>\n",
       "      <td>Progression free survival</td>\n",
       "      <td>Up to 2 years</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT01622868</td>\n",
       "      <td>Overall CNS complete response</td>\n",
       "      <td>Up to 2 years</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT02166957</td>\n",
       "      <td>Number of endoscopic sessions</td>\n",
       "      <td>Up to 2 years</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT02508077</td>\n",
       "      <td>Impact of time since last anti-EGFR exposure o...</td>\n",
       "      <td>Up to 2 years</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00275093</td>\n",
       "      <td>Antitumor efficacy</td>\n",
       "      <td>Up to 30 days</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT01001754</td>\n",
       "      <td>Serum drug concentration profile</td>\n",
       "      <td>Up to week 48</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00334139</td>\n",
       "      <td>Pain</td>\n",
       "      <td>every 30 days</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00152867</td>\n",
       "      <td>Differences in nausea and vomiting by treatmen...</td>\n",
       "      <td>post period 2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00803231</td>\n",
       "      <td>Length of hospital stay</td>\n",
       "      <td>28 and 31 days</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT02961634</td>\n",
       "      <td>Number of patients reporting reduction in disc...</td>\n",
       "      <td>45 and 90 days</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                            measure  \\\n",
       "0  NCT01644097                          Progression free survival   \n",
       "1  NCT01622868                      Overall CNS complete response   \n",
       "2  NCT02166957                      Number of endoscopic sessions   \n",
       "3  NCT02508077  Impact of time since last anti-EGFR exposure o...   \n",
       "4  NCT00275093                                 Antitumor efficacy   \n",
       "5  NCT01001754                   Serum drug concentration profile   \n",
       "6  NCT00334139                                               Pain   \n",
       "7  NCT00152867  Differences in nausea and vomiting by treatmen...   \n",
       "8  NCT00803231                            Length of hospital stay   \n",
       "9  NCT02961634  Number of patients reporting reduction in disc...   \n",
       "\n",
       "       time_frame  safety_issue description  count  \n",
       "0   Up to 2 years         False        None      2  \n",
       "1   Up to 2 years         False        None      2  \n",
       "2   Up to 2 years         False        None      2  \n",
       "3   Up to 2 years         False        None      2  \n",
       "4   Up to 30 days         False        None      2  \n",
       "5   Up to week 48         False        None      2  \n",
       "6   every 30 days         False        None      2  \n",
       "7   post period 2         False        None      2  \n",
       "8  28 and 31 days         False        None      2  \n",
       "9  45 and 90 days         False        None      2  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, measure, time_frame, safety_issue, description, count(nct_number) as count\n",
    "from nih_modeled.secondary_outcomes\n",
    "group by nct_number, measure, time_frame, safety_issue, description\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Outcomes Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  83422\n",
       "1  14397"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.other_outcomes oo\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.other_outcomes oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>measure</th>\n",
       "      <th>time_frame</th>\n",
       "      <th>safety_issue</th>\n",
       "      <th>description</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00000372</td>\n",
       "      <td>Change in Brief Psychotic Rating Scale from Ba...</td>\n",
       "      <td>Baseline, Week 8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00000372</td>\n",
       "      <td>Change in Global Assessment Scale from Baselin...</td>\n",
       "      <td>Baseline, Week 8</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00000572</td>\n",
       "      <td>Hospital costs</td>\n",
       "      <td>Hostpital stay</td>\n",
       "      <td>False</td>\n",
       "      <td>$US</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00000572</td>\n",
       "      <td>Bleeding/Hemorrhage</td>\n",
       "      <td>Hospital stay</td>\n",
       "      <td>True</td>\n",
       "      <td>Bleeding in ECCO2R subjects exceeded that in c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00003193</td>\n",
       "      <td>Survival</td>\n",
       "      <td>5 years</td>\n",
       "      <td>False</td>\n",
       "      <td>Percentage of patients alive at 5 years as wel...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCT00003224</td>\n",
       "      <td>Number of Participants With a Proliferative Re...</td>\n",
       "      <td>during vaccination</td>\n",
       "      <td>False</td>\n",
       "      <td>Proliferative response measured in participant...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NCT00004195</td>\n",
       "      <td>Specific Aims</td>\n",
       "      <td>Duriation of trial up to 30 days after first dose</td>\n",
       "      <td>True</td>\n",
       "      <td>Determine the enzymatic activity of DPD in PBM...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NCT00004418</td>\n",
       "      <td>very long chain fatty acids (VLCFA)</td>\n",
       "      <td>every month during first year, then every 1-3 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NCT00005275</td>\n",
       "      <td>Prevalent cardiovascular events and stroke</td>\n",
       "      <td>1998-2011</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NCT00005349</td>\n",
       "      <td>coronary artery disease risk factors</td>\n",
       "      <td>1991-2005</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nct_number                                            measure  \\\n",
       "0  NCT00000372  Change in Brief Psychotic Rating Scale from Ba...   \n",
       "1  NCT00000372  Change in Global Assessment Scale from Baselin...   \n",
       "2  NCT00000572                                     Hospital costs   \n",
       "3  NCT00000572                                Bleeding/Hemorrhage   \n",
       "4  NCT00003193                                           Survival   \n",
       "5  NCT00003224  Number of Participants With a Proliferative Re...   \n",
       "6  NCT00004195                                      Specific Aims   \n",
       "7  NCT00004418                very long chain fatty acids (VLCFA)   \n",
       "8  NCT00005275         Prevalent cardiovascular events and stroke   \n",
       "9  NCT00005349               coronary artery disease risk factors   \n",
       "\n",
       "                                          time_frame  safety_issue  \\\n",
       "0                                   Baseline, Week 8         False   \n",
       "1                                   Baseline, Week 8         False   \n",
       "2                                     Hostpital stay         False   \n",
       "3                                      Hospital stay          True   \n",
       "4                                            5 years         False   \n",
       "5                                 during vaccination         False   \n",
       "6  Duriation of trial up to 30 days after first dose          True   \n",
       "7  every month during first year, then every 1-3 ...         False   \n",
       "8                                          1998-2011         False   \n",
       "9                                          1991-2005         False   \n",
       "\n",
       "                                         description  count  \n",
       "0                                               None      2  \n",
       "1                                               None      2  \n",
       "2                                                $US      2  \n",
       "3  Bleeding in ECCO2R subjects exceeded that in c...      2  \n",
       "4  Percentage of patients alive at 5 years as wel...      2  \n",
       "5  Proliferative response measured in participant...      2  \n",
       "6  Determine the enzymatic activity of DPD in PBM...      2  \n",
       "7                                               None      2  \n",
       "8                                               None      2  \n",
       "9                                               None      2  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, measure, time_frame, safety_issue, description, count(nct_number) as count\n",
    "from nih_modeled.other_outcomes\n",
    "group by nct_number, measure, time_frame, safety_issue, description\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arm Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Interactive Beam requires Python 3.5.3+.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'arm_groups'> referenced by query SELECT nct_number, arm_group_label, arm_group_type, description, serialid FROM nih_modeled.arm_groups limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_ef75159a9cb9471590b88c6f4bbf336f does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.11 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.arm_groups_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_label'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583781064313\n",
      " etag: '0zujvJJSjzEM8TWaGQbUUg=='\n",
      " id: 'probable-pager-266720:nih_modeled.arm_groups_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781064366\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_label'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/arm_groups_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'arm_groups_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run arm_groups_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpaf8979x5', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpaf8979x5', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/arm-groups-df.1583783040.818988/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T19:44:09.150440Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_12_44_07-5914124653428320430'\n",
      " location: 'us-central1'\n",
      " name: 'arm-groups-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T19:44:09.150440Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_12_44_07-5914124653428320430]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_12_44_07-5914124653428320430?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_12_44_07-5914124653428320430 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:07.965Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_12_44_07-5914124653428320430.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:07.965Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_12_44_07-5914124653428320430. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:11.436Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:12.335Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:12.996Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.024Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.045Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.109Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.142Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.170Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.271Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.646Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.674Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.706Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.733Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.764Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.791Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.817Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.850Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.875Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.925Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.956Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:13.983Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.044Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.075Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.101Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.123Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.145Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.174Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.197Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.222Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.246Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.271Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.294Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.318Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.344Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.369Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.397Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.543Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_12_44_07-5914124653428320430 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.607Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.641Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.652Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.666Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.679Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.696Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.722Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.734Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.747Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.774Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.787Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.813Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:14.836Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:44:41.309Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:12.562Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:12.587Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.447Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.517Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.548Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.616Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.639Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.663Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.675Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.695Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.710Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.733Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.762Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:47.788Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:50.886Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:50.939Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.036Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.100Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.126Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.149Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.151Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.168Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.193Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.212Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.239Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.261Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.300Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:46:51.403Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_781954408734111080\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_781954408734111080\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:48:11.380Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_781954408734111080\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:48:11.778Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_10812700304920002554\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_10812700304920002554\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:48:42.167Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_10812700304920002554\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:48:42.201Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_10812700304920002554\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.071Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.126Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.154Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.175Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.203Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.236Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:49:39.260Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:14.522Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:24.621Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_781954408734112604\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_781954408734112604\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.408Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.470Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.526Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.552Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.569Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.604Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.615Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.658Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:28.719Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:32.788Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:32.877Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:32.951Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:33.004Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:33.079Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:33.147Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:35.156Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_781954408734112604\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:35.607Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:35.654Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:35.698Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:35.746Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:36.615Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.399Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.452Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.514Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.542Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.565Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.585Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.618Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.660Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:46.853Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:50.906Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:50.973Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:51.027Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:51.076Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:51.132Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:51.194Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:52.448Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:52.524Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:52.652Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:52.745Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:50:52.779Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:52:59.374Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:52:59.414Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T19:52:59.443Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_12_44_07-5914124653428320430 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run arm_groups_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  191171\n",
       "1  191171"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.arm_groups_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.arm_groups_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.arm_groups_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.arm_groups_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'clinical_results'> referenced by query SELECT nct_number, type, title, time_frame, safety_issue, results_population, serialid FROM nih_modeled.clinical_results limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_b8c19ade353742a98c1c2bb8928ceab4 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.clinical_results_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'results_population'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583781220484\n",
      " etag: 'UGAnJiD4us5LUPng22j72Q=='\n",
      " id: 'probable-pager-266720:nih_modeled.clinical_results_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781220514\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'results_population'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/clinical_results_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'clinical_results_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run clinical_results_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_xg4ylf5', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp_xg4ylf5', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-results-df.1583784132.938183/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T20:02:19.337699Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_13_02_17-17717194396415184583'\n",
      " location: 'us-central1'\n",
      " name: 'clinical-results-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T20:02:19.337699Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_13_02_17-17717194396415184583]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_13_02_17-17717194396415184583?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_02_17-17717194396415184583 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:17.667Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_13_02_17-17717194396415184583. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:17.667Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_13_02_17-17717194396415184583.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:22.249Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:22.948Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.577Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.609Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.642Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.664Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.704Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.738Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:23.873Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.115Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.138Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.167Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.199Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.234Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.268Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.304Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.338Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.364Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.395Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.429Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.466Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.503Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.539Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.577Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.609Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.646Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.679Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.711Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.747Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.781Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.816Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.849Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.883Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.916Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.951Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:24.984Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.142Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.202Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.229Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.254Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.262Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.285Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.296Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.330Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.351Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.353Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.379Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.429Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.464Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:25.487Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_02_17-17717194396415184583 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:02:51.854Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:04:36.210Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:04:36.236Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:29.953Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.018Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.055Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.123Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.158Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.180Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.199Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.226Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.257Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.288Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.299Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.354Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.390Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:30.457Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_15200115950655671957\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15200115950655671957\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.483Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.565Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.598Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.663Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.692Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.715Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.728Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.748Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.776Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.785Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.824Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:05:33.857Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:06:49.133Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_15200115950655671957\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:06:49.756Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_7587676467222264993\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_7587676467222264993\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:07:20.200Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_7587676467222264993\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:07:20.270Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_7587676467222264993\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.761Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.831Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.876Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.884Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.933Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:24.961Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:25.004Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:25.116Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.384Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.446Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.520Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.585Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.617Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.641Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.679Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.701Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:44.771Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 2/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:54.725Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:54.780Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:54.841Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:54.890Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:54.949Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:08:55.007Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:00.309Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_15200115950655668337\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15200115950655668337\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:03.437Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:10.915Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_15200115950655668337\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:11.573Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:11.644Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:11.689Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:11.742Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.765Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.828Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.912Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.938Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.965Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:19.980Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:20.022Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:20.044Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:20.106Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:23.793Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:23.854Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:23.910Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:23.953Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:24.008Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:24.063Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:27.457Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:27.530Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:27.635Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:27.689Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:09:27.718Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:11:22.424Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:11:22.467Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:11:22.493Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_02_17-17717194396415184583 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run clinical_results_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  23895\n",
       "1  23895"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.clinical_results_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.clinical_results_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.clinical_results_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.clinical_results_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Studies Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'clinical_studies_main'> referenced by query SELECT nct_number, org_study_id, secondary_id, official_title, brief_summary, overall_status, enrollment, enrollment_type, start_date, competion_date, condition, number_of_arms, number_of_groups, phase, study_type, study_design, first_recieved_date, verification_date, lead_sponsor_agency, lead_sponsor_agency_class, official_full_name, overall_official_affiliation, serialid FROM nih_modeled.clinical_studies_main limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_e198a0de897e447cbb30e046c61b5657 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.clinical_studies_main_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'org_study_id'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'secondary_id'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'official_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'brief_summary'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'overall_status'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'enrollment'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'enrollment_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'start_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'competion_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'condition'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'number_of_arms'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'number_of_groups'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phase'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'study_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'study_design'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'first_recieved_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'verification_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'lead_sponsor_agency'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'lead_sponsor_agency_class'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'official_full_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'overall_official_affiliation'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583781376603\n",
      " etag: 'r2WkVGPxLOP1BcT8YRQTng=='\n",
      " id: 'probable-pager-266720:nih_modeled.clinical_studies_main_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781376655\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'org_study_id'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'secondary_id'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'official_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'brief_summary'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'overall_status'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'enrollment'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'enrollment_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'start_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'competion_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'condition'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'number_of_arms'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'number_of_groups'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phase'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'study_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'study_design'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'first_recieved_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'verification_date'\n",
      " type: 'DATE'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'lead_sponsor_agency'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'lead_sponsor_agency_class'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'official_full_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'overall_official_affiliation'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/clinical_studies_main_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'clinical_studies_main_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run clinical_studies_main_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmphpbhopky', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmphpbhopky', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/clinical-studies-main-df.1583786429.384121/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T20:40:35.012451Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_13_40_34-17735194750731332231'\n",
      " location: 'us-central1'\n",
      " name: 'clinical-studies-main-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T20:40:35.012451Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_13_40_34-17735194750731332231]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_13_40_34-17735194750731332231?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_40_34-17735194750731332231 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:34.066Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_13_40_34-17735194750731332231.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:34.066Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_13_40_34-17735194750731332231. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:37.280Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:38.999Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.452Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.478Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.498Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.533Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.566Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.594Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.692Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.941Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:39.973Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.031Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.065Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.092Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.122Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.148Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.179Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.203Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.232Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.255Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.285Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.317Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_40_34-17735194750731332231 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.346Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.368Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.397Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.419Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.445Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.472Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.493Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.518Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.544Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.571Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.596Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.618Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.646Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:40.993Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.061Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.089Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.106Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.114Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.131Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.137Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.181Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.181Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.201Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.239Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.243Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.265Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:40:41.303Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:41:06.067Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 2/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:42:36.192Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:42:36.241Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:17.682Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:17.881Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:17.911Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:17.960Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:17.993Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.007Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.019Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.046Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.062Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.070Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.102Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:18.132Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.012Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.088Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.122Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.190Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.221Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.249Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.258Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.272Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.304Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.312Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.336Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.365Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.401Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:43:21.499Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_11383572161561388038\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_11383572161561388038\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:44:58.486Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_11383572161561388038\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:44:59.065Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_14348732848700683712\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_14348732848700683712\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:45:59.541Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_14348732848700683712\" observed total of 2 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:45:59.572Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_14348732848700683712\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:46:40.880Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:28.405Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:28.626Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:28.750Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:28.833Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:28.940Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:29.007Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:49:29.109Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:52.972Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.036Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.111Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.143Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.165Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.188Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.230Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.261Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:50:53.329Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:37.513Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_11383572161561390466\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_11383572161561390466\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:40.721Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:40.790Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:40.848Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:40.895Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:40.961Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:41.020Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:51:44.397Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:18.439Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_11383572161561390466\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:19.062Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:19.127Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:19.173Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:19.233Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.193Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.249Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.315Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.345Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.364Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.394Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.410Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.452Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:26.514Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:30.296Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:30.460Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:30.726Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:30.823Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:30.985Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:31.079Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:33.205Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:33.293Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:33.425Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:33.504Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:52:33.546Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:54:47.003Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:54:47.047Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:54:47.076Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_40_34-17735194750731332231 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run clinical_studies_main_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary Key and Duplicate Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  234703\n",
       "1  234703"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.clinical_studies_main_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.clinical_studies_main_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.clinical_studies_main_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'collaborators'> referenced by query SELECT nct_number, collaborator_agency, collaborator_agency_class, serialid FROM nih_modeled.collaborators limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_d0cb96485548495888b1f4cfa04e1459 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.collaborators_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'collaborator_agency'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'collaborator_agency_class'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583781533813\n",
      " etag: 'tCW59LyNWwZGPN1uAXi3+g=='\n",
      " id: 'probable-pager-266720:nih_modeled.collaborators_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781533852\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'collaborator_agency'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'collaborator_agency_class'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/collaborators_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'collaborators_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run collaborators_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpqe141lik', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpqe141lik', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/collaborators-df.1583787296.590130/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T20:55:02.327155Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_13_55_01-2556987110398693906'\n",
      " location: 'us-central1'\n",
      " name: 'collaborators-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T20:55:02.327155Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_13_55_01-2556987110398693906]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_13_55_01-2556987110398693906?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_55_01-2556987110398693906 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:01.375Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_13_55_01-2556987110398693906. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:01.375Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_13_55_01-2556987110398693906.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:06.825Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:08.579Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-f.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.143Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.191Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.233Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.287Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.417Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.459Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:09.672Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.049Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.084Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.134Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.180Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.247Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.287Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.351Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.444Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.524Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.570Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.627Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.697Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.767Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.838Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.891Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.941Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:10.985Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.027Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.071Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.117Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.150Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.197Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.247Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.318Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.375Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.422Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.469Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.796Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.884Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.920Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.932Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.957Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.978Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-f...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:11.991Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.032Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.067Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.067Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.095Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.142Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.184Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:12.282Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_55_01-2556987110398693906 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:55:43.140Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:15.178Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:15.237Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:48.790Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:48.920Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:48.981Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.098Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.161Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.201Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.233Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.306Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.339Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.366Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.450Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.500Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.559Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:49.812Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_202646073583578929\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_202646073583578929\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:52.421Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:52.645Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:52.708Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:52.885Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:52.954Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.004Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.021Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.063Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.121Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.177Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.256Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:57:53.296Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:58:59.244Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_202646073583578929\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:58:59.880Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_1752845433598180586\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_1752845433598180586\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:30.279Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_1752845433598180586\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:30.368Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_1752845433598180586\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:51.741Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:51.882Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:51.919Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:51.965Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:51.999Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:52.053Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T20:59:52.113Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.393Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.617Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.773Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.867Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.915Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:15.955Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:16.104Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:16.176Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:16.349Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:20.287Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_202646073583577781\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_202646073583577781\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:23.710Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:23.833Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:24.021Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:24.115Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:24.226Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:24.394Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:28.085Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:30.992Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_202646073583577781\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:31.614Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:31.766Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:31.870Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:32.001Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:39.466Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:39.717Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:39.885Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:39.976Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:40.011Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:40.117Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:40.151Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:40.245Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:40.409Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:44.666Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:44.727Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:44.836Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:44.948Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:45.028Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:45.108Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:46.746Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:46.851Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:47.144Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:47.287Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:00:47.355Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:21.207Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:21.276Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:21.322Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_13_55_01-2556987110398693906 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run collaborators_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  78684\n",
       "1  78684"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.collaborators_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.collaborators_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.collaborators_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.collaborators_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'contacts'> referenced by query SELECT nct_number, last_name, phone, phone_ext, email, serialid FROM nih_modeled.contacts limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_61c965166d4c49d291ebf4241d7d2621 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.contacts_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'last_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phone'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phone_ext'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'email'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583781689815\n",
      " etag: 'W47fyaZY2hIjyd3xHkzl/w=='\n",
      " id: 'probable-pager-266720:nih_modeled.contacts_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781689851\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'last_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phone'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'phone_ext'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'email'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/contacts_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'contacts_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run contacts_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp6ctopgw4', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp6ctopgw4', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/contacts-df.1583787748.502106/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T21:02:34.293700Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_14_02_33-17532175654245271654'\n",
      " location: 'us-central1'\n",
      " name: 'contacts-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T21:02:34.293700Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_14_02_33-17532175654245271654]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_14_02_33-17532175654245271654?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_02_33-17532175654245271654 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:33.102Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_14_02_33-17532175654245271654. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:33.102Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_14_02_33-17532175654245271654.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:36.813Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:37.724Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.278Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.316Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.351Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.390Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.416Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.453Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.595Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.846Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.880Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.911Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.940Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:38.971Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.009Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.047Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.071Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.110Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.153Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.192Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.230Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.266Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.302Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.345Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.380Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.411Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.447Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.485Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.521Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.551Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.580Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.623Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.662Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.705Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.733Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:39.773Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.264Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.347Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.388Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.390Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.423Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.427Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.472Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.487Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.508Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.535Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.573Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.585Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.622Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:02:40.660Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_02_33-17532175654245271654 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:03:09.501Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:04:52.950Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:04:52.988Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.619Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.724Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.798Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.867Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.909Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.932Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.947Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:42.973Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.012Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.017Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.053Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.118Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.161Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:43.313Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_15579295232570795012\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15579295232570795012\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.272Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.359Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.391Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.482Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.522Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.545Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.564Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.602Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.631Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.644Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.697Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:05:46.745Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:07:22.550Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_15579295232570795012\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:07:23.038Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_10384270044277548205\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_10384270044277548205\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:07:53.507Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_10384270044277548205\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:07:53.546Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_10384270044277548205\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:37.981Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.073Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.115Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.138Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.168Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.232Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:38.268Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:08:40.233Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:05.346Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_15579295232570797040\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15579295232570797040\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.163Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.228Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.298Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.328Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.355Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.378Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.434Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.458Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:08.516Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.277Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.341Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.402Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.453Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.542Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:12.616Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:15.900Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:15.986Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_15579295232570797040\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:16.416Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:16.494Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:16.559Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:16.628Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.711Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.789Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.917Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.948Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.975Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:26.993Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:27.094Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:27.215Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:27.427Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.428Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.512Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.574Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.636Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.706Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:31.779Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:35.084Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:35.155Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:35.313Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:35.391Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:09:35.431Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 2/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:19.789Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:19.826Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:19.854Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_02_33-17532175654245271654 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run contacts_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  37569\n",
       "1  37569"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.contacts_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.contacts_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.contacts_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.contacts_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'eligibility'> referenced by query SELECT nct_number, eligibility_study_pop, eligibility_sampling_method, eligibility_criteria, eligibility_gender, eligibility_minimum_age, eligibility_maximum_age, eligibility_healthy_volunteers FROM nih_modeled.eligibility limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_ecb61e7e0f5345ad8fe7e4ed34801328 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.eligibility_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_study_pop'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_sampling_method'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_criteria'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_gender'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_minimum_age'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_maximum_age'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_healthy_volunteers'\n",
      " type: 'STRING'>]>. Result: <Table\n",
      " creationTime: 1583781845568\n",
      " etag: 'LXWU6XPV8cDgKDX1I/CDcQ=='\n",
      " id: 'probable-pager-266720:nih_modeled.eligibility_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583781845603\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_study_pop'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_sampling_method'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_criteria'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_gender'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_minimum_age'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_maximum_age'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'eligibility_healthy_volunteers'\n",
      " type: 'STRING'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/eligibility_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'eligibility_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run eligibility_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpvucc80t8', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpvucc80t8', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/eligibility-df.1583788285.544575/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T21:11:31.035344Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_14_11_29-7758953295381268765'\n",
      " location: 'us-central1'\n",
      " name: 'eligibility-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T21:11:31.035344Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_14_11_29-7758953295381268765]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_14_11_29-7758953295381268765?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_11_29-7758953295381268765 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:30.050Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_14_11_29-7758953295381268765. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:30.050Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_14_11_29-7758953295381268765.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:33.342Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:33.895Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-f.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.510Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.550Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.584Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.618Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.664Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.691Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:34.843Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.106Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.144Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.167Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.201Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.234Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.277Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.321Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.362Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.402Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.437Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.472Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.508Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.571Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.607Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.646Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.684Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.718Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.757Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.784Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.823Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.863Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.895Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.926Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:35.966Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.039Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.068Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_11_29-7758953295381268765 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.448Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.534Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.565Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.577Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.596Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.613Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-f...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.629Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.666Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.675Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.675Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.721Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.738Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.774Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:11:36.807Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:12:06.951Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:13:33.535Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:13:33.566Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.743Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.813Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.843Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.907Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.933Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.960Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.961Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:09.989Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:10.010Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:10.014Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:10.052Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:10.077Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.365Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.464Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.487Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.645Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.675Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.698Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.706Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.727Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.751Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.763Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.784Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.821Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.849Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:14:13.947Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_7876994328265463857\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_7876994328265463857\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:16:01.948Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_7876994328265463857\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:16:02.391Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_5428409786657649160\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_5428409786657649160\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:17:02.989Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_5428409786657649160\" observed total of 3 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:17:03.053Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_5428409786657649160\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:17:36.427Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:22.926Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:22.983Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:23.012Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:23.030Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:23.057Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:23.093Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:19:23.120Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.005Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.061Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.117Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.144Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.168Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.192Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.223Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.247Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:09.298Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.627Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.706Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.783Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.831Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.906Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:20:40.966Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:21:20.397Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_7876994328265463757\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_7876994328265463757\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:21:21.691Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:01.953Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_7876994328265463757\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:02.441Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:02.517Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:02.581Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:02.654Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:08.761Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:08.845Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:08.921Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:08.961Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:08.982Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:09.024Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:09.049Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:09.095Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:09.158Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.026Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.106Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.181Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.234Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.310Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:12.381Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:14.609Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:14.687Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:14.818Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:14.876Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:22:14.911Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:27.517Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:27.549Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:27.575Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_11_29-7758953295381268765 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run eligibility_beam_dataflow.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  234703\n",
       "1  234703"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.eligibility_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.eligibility_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.eligibility_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.eligibility_Beam elb LEFT JOIN nih_modeled.clinical_studies_main csm ON elb.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'interventions'> referenced by query SELECT nct_number, intervention_type, intervention_name, arm_group_label, serialid FROM nih_modeled.interventions limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_15866a4d8518401887b4bfb6faac3d3a does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.interventions_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'intervention_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'intervention_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_label'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782001489\n",
      " etag: '/ohMGcWnXeaI/Q9/qsKlfw=='\n",
      " id: 'probable-pager-266720:nih_modeled.interventions_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782001524\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'intervention_type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'intervention_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'arm_group_label'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/interventions_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'interventions_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run interventions_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpxduk_qjd', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpxduk_qjd', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/interventions-df.1583789023.754855/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T21:23:49.675052Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_14_23_48-8126933974229270253'\n",
      " location: 'us-central1'\n",
      " name: 'interventions-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T21:23:49.675052Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_14_23_48-8126933974229270253]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_14_23_48-8126933974229270253?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_23_48-8126933974229270253 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:48.668Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_14_23_48-8126933974229270253. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:48.669Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_14_23_48-8126933974229270253.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:52.521Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.084Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.730Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.823Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.860Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.884Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.933Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:53.967Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.115Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.434Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.472Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.509Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.538Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.571Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.597Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.637Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.662Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.690Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.720Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.750Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.788Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.819Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.854Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.884Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.908Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.934Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.960Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:54.991Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.013Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.044Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.076Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.111Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.151Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.185Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.214Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.242Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.509Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.608Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.646Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.656Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.676Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.680Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.710Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.740Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.759Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.759Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.803Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.831Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.871Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:23:55.908Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_23_48-8126933974229270253 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:24:22.246Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:02.416Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:02.444Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.166Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.237Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.264Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.328Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.359Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.383Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.386Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.437Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.443Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.450Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.507Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:38.530Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.587Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.643Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.674Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.738Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.770Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.785Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.800Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.819Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.833Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.855Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.882Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.916Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:41.953Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:26:42.123Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_15899079296444587086\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15899079296444587086\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:27:59.991Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_15899079296444587086\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:28:01.998Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_7124322478839863353\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_7124322478839863353\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:28:32.375Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_7124322478839863353\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:28:32.407Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_7124322478839863353\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.231Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.291Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.320Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.329Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.370Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.381Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.427Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:29:55.460Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.598Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.679Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.746Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.784Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.802Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.834Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.869Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.903Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:32.965Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:46.475Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_15899079296444584762\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15899079296444584762\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.674Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.746Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.808Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.858Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.920Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:49.994Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:53.509Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:57.196Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_15899079296444584762\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:57.735Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:57.839Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:57.882Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:30:57.941Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:05.887Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:05.945Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.002Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.028Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.057Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.087Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.126Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.157Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:06.218Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.182Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.245Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.300Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.350Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.403Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:10.468Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:12.541Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:12.600Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:12.716Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:12.785Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:31:12.818Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:33:00.884Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:33:00.926Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:33:00.955Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_23_48-8126933974229270253 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run interventions_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  207014\n",
       "1  207014"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.interventions_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.interventions_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.interventions_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.interventions_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'locations'> referenced by query SELECT nct_number, facility_name, facility_city, facility_state, facility_zip, facility_country, serialid FROM nih_modeled.locations limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_e9b6500db9d54a66a73d354c06342fde does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.locations_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_city'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_state'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_zip'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_country'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782157298\n",
      " etag: 'tJh27yIMSxW4mTxXMqLRXA=='\n",
      " id: 'probable-pager-266720:nih_modeled.locations_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782157333\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_city'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_state'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_zip'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'facility_country'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/locations_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'locations_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run locations_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Interactive Beam requires Python 3.5.3+.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/pipeline.pb...\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp7_xx3as6', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp7_xx3as6', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/locations-df.1583805878.669935/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-10T02:04:46.499924Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_19_04_45-14155125391451087048'\n",
      " location: 'us-central1'\n",
      " name: 'locations-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-10T02:04:46.499924Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_19_04_45-14155125391451087048]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_19_04_45-14155125391451087048?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_19_04_45-14155125391451087048 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:45.224Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_19_04_45-14155125391451087048.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:45.224Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_19_04_45-14155125391451087048. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:49.817Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:51.443Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:51.998Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.038Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.128Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.169Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.211Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.244Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.394Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.770Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.807Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.845Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.875Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.908Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.943Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:52.981Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.015Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.053Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.089Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.175Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.212Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.236Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.268Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.306Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.343Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.367Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.398Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.434Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.472Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.507Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.542Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.579Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.617Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.651Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.685Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.723Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.907Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:53.985Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.025Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.037Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.055Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.067Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.089Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.127Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.139Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.156Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.178Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.228Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.262Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:04:54.300Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_19_04_45-14155125391451087048 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:05:21.243Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:10.763Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:10.800Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.069Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.141Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.175Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.275Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.310Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.344Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.351Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.365Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.411Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.427Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.464Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:46.495Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.560Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.622Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.646Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.715Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.784Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.812Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.818Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.837Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.873Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.901Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.945Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:49.982Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:50.025Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:07:50.165Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_15613912967059780562\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_15613912967059780562\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:09:54.611Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_15613912967059780562\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:09:55.023Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_13321430342106739081\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_13321430342106739081\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:10:25.444Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_13321430342106739081\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:10:25.480Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_13321430342106739081\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:10:53.868Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:12:00.278Z: JOB_MESSAGE_BASIC: Autoscaling: Resizing worker pool from 1 to 2.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:12:06.097Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 2 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:05.371Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/43651fcf-2fc9-4542-9d43-3c8a824707a6.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/43651fcf-2fc9-4542-9d43-3c8a824707a6.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:08.647Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/988fcfbb-65e1-4b43-b5a0-fc2e62c50ee8.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/988fcfbb-65e1-4b43-b5a0-fc2e62c50ee8.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:12.912Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/12a46031-f007-4333-ad2a-229e234acc3f.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/12a46031-f007-4333-ad2a-229e234acc3f.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:28.366Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/9b572404-4c13-423e-9875-68d52cb8b216.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/9b572404-4c13-423e-9875-68d52cb8b216.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:31.460Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/2fe66e0e-0ab8-433a-a0fb-2ffc44c16a13.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/2fe66e0e-0ab8-433a-a0fb-2ffc44c16a13.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:34.692Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/7cf67dd4-dc7e-44f1-9f94-3bab35aaf04f.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/7cf67dd4-dc7e-44f1-9f94-3bab35aaf04f.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:38.005Z: JOB_MESSAGE_ERROR: Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/fe009a19-507f-4961-81e6-c2193177b35b.input.txt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 648, in do_work\n",
      "    work_executor.execute()\n",
      "  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n",
      "    op.start()\n",
      "  File \"dataflow_worker/native_operations.py\", line 38, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 39, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 44, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"dataflow_worker/native_operations.py\", line 54, in dataflow_worker.native_operations.NativeReadOperation.start\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 304, in apache_beam.runners.worker.operations.Operation.output\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 131, in apache_beam.runners.worker.operations.ConsumerSet.receive\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 657, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/worker/operations.py\", line 658, in apache_beam.runners.worker.operations.DoOperation.process\n",
      "  File \"apache_beam/runners/common.py\", line 878, in apache_beam.runners.common.DoFnRunner.receive\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/usr/local/lib/python3.5/site-packages/future/utils/__init__.py\", line 421, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 667, in apache_beam.runners.common.PerWindowInvoker.invoke_process\n",
      "  File \"apache_beam/runners/common.py\", line 748, in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/iobase.py\", line 1057, in process\n",
      "    self.writer = self.sink.open_writer(init_result, str(uuid.uuid4()))\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 191, in open_writer\n",
      "    return FileBasedSinkWriter(self, writer_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 395, in __init__\n",
      "    self.temp_handle = self.sink.open(temp_shard_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/textio.py\", line 397, in open\n",
      "    file_handle = super(_TextSink, self).open(temp_path)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/options/value_provider.py\", line 140, in _f\n",
      "    return fnc(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filebasedsink.py\", line 134, in open\n",
      "    return FileSystems.create(temp_path, self.mime_type, self.compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/filesystems.py\", line 217, in create\n",
      "    return filesystem.create(path, mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 155, in create\n",
      "    return self._path_open(path, 'wb', mime_type, compression_type)\n",
      "  File \"/usr/local/lib/python3.5/site-packages/apache_beam/io/localfilesystem.py\", line 137, in _path_open\n",
      "    raw_file = open(path, mode)\n",
      "RuntimeError: FileNotFoundError: [Errno 2] No such file or directory: '/beam-temp-input.txt-ef876980627311eabb9f42010a800017/fe009a19-507f-4961-81e6-c2193177b35b.input.txt' [while running 'Write records input/Write/WriteImpl/WriteBundles/WriteBundles']\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:38.088Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:38.164Z: JOB_MESSAGE_DEBUG: Executing failure step failure38\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:38.208Z: JOB_MESSAGE_ERROR: Workflow failed. Causes: S08:Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write failed., The job failed because a work item has failed 4 times. Look in previous log entries for the cause of each one of the 4 failures. For more information, see https://cloud.google.com/dataflow/docs/guides/common-errors. The work item was attempted on these workers: \n",
      "  locations-df-03091904-6fgu-harness-cj0v\n",
      "      Root cause: Work item failed.,\n",
      "  locations-df-03091904-6fgu-harness-cj0v\n",
      "      Root cause: Work item failed.,\n",
      "  locations-df-03091904-6fgu-harness-cj0v\n",
      "      Root cause: Work item failed.,\n",
      "  locations-df-03091904-6fgu-harness-cj0v\n",
      "      Root cause: Work item failed.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:39.467Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:39.654Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-10T02:14:39.690Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n"
     ]
    }
   ],
   "source": [
    "%run locations_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.locations_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.locations_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.locations_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.locations_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Outcomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'other_outcomes'> referenced by query SELECT nct_number, measure, time_frame, safety_issue, description, serialid FROM nih_modeled.other_outcomes limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_c99ad4c895044f6c8ae8e93f1d9a1d2e does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.other_outcomes_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782313372\n",
      " etag: 's6nN07InRW7cSacFFH7rEw=='\n",
      " id: 'probable-pager-266720:nih_modeled.other_outcomes_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782313419\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/other_outcomes_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'other_outcomes_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run other_outcomes_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpsein_thq', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpsein_thq', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/other-outcomes-df.1583790288.204691/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T21:44:53.705904Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_14_44_52-5101239610093867591'\n",
      " location: 'us-central1'\n",
      " name: 'other-outcomes-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T21:44:53.705904Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_14_44_52-5101239610093867591]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_14_44_52-5101239610093867591?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_44_52-5101239610093867591 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:52.784Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_14_44_52-5101239610093867591. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:52.785Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_14_44_52-5101239610093867591.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:56.216Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:56.762Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.297Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.333Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.364Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.389Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.429Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.466Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.615Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.844Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.878Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.901Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.933Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:57.970Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.033Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.064Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.101Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.138Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.179Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.216Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.261Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.289Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.323Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.358Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.403Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.473Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.512Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.547Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.579Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.622Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.660Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.700Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.743Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.777Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:58.818Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_44_52-5101239610093867591 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.041Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.124Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.161Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.173Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.192Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.205Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.227Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.269Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.280Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.336Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.352Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.362Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.415Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:44:59.451Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:45:29.276Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:47:17.334Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:47:17.406Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.019Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.098Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.136Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.211Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.251Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.277Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.284Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.312Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.344Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.348Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.380Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:04.415Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.425Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.492Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.521Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.579Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.616Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.630Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.652Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.671Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.700Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.708Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.747Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.783Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.815Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:48:07.913Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_2925773872777686098\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_2925773872777686098\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:49:43.879Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_2925773872777686098\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:49:44.337Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_12584781543086805772\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_12584781543086805772\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:14.687Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_12584781543086805772\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:14.717Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_12584781543086805772\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.431Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.576Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.617Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.644Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.689Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.728Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:43.781Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:50:59.015Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:06.785Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:06.858Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:06.950Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:06.990Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:07.012Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:07.083Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:07.129Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:07.154Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:07.229Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:11.536Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_2925773872777683454\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_2925773872777683454\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:15.903Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:16.005Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:16.104Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:16.189Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:16.284Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:16.362Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:19.651Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:22.272Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_2925773872777683454\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:22.821Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:22.901Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:22.976Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:23.055Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.340Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.411Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.482Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.514Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.538Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.570Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.621Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.659Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:33.727Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:37.639Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:37.773Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:37.835Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:37.885Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:38.076Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:38.185Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:41.979Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:42.055Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:42.193Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:42.256Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:51:42.281Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:53:45.678Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:53:45.717Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:53:45.743Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_44_52-5101239610093867591 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run other_outcomes_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0  14397\n",
       "1  14397"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.other_outcomes_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.other_outcomes_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.other_outcomes_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.other_outcomes_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'primary_outcomes'> referenced by query SELECT nct_number, measure, time_frame, safety_issue, description, serialid FROM nih_modeled.primary_outcomes limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_6ba91422135742cbaa9a2ee27f0aa042 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.primary_outcomes_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782469831\n",
      " etag: 'tkfpBNOSxEOV0+VGD0H1Rw=='\n",
      " id: 'probable-pager-266720:nih_modeled.primary_outcomes_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782469897\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/primary_outcomes_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'primary_outcomes_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run primary_outcomes_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5mrttd88', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp5mrttd88', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/primary-outcomes-df.1583790834.982337/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T21:54:00.513745Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_14_53_59-3013522702870427948'\n",
      " location: 'us-central1'\n",
      " name: 'primary-outcomes-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T21:54:00.513745Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_14_53_59-3013522702870427948]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_14_53_59-3013522702870427948?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_53_59-3013522702870427948 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:53:59.613Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_14_53_59-3013522702870427948.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:53:59.613Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_14_53_59-3013522702870427948. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:02.736Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:03.339Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-f.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:03.988Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.087Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.115Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.142Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.175Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.199Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.297Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.600Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.623Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.650Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.669Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.692Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.712Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.738Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.766Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.793Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.820Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.848Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.873Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.903Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.929Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.956Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:04.980Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.009Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.034Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.058Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.085Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.110Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.137Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.158Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.179Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.203Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.226Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.250Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.408Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.463Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.485Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.497Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.511Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.520Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-f...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.539Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.566Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.569Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.586Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.615Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.625Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.656Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:05.685Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_53_59-3013522702870427948 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:54:31.876Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:11.047Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:11.098Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:51.837Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:51.893Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:51.972Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.039Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.072Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.100Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.100Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.126Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.160Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.164Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.186Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:52.226Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.420Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.487Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.510Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.557Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.585Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.614Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.618Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.638Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.665Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.672Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.689Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.727Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.747Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:56:55.850Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_3840625137528756779\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_3840625137528756779\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:58:21.498Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_3840625137528756779\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:58:21.999Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_14859132221731807420\" started. You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_14859132221731807420\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:58:52.519Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_14859132221731807420\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T21:58:52.556Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_14859132221731807420\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:05.394Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.146Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Extract table records+Write records input/Write/WriteImpl/WriteBundles/WriteBundles+Write records input/Write/WriteImpl/Pair+Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write records input/Write/WriteImpl/GroupByKey/Reify+Write records input/Write/WriteImpl/GroupByKey/Write+Grouped table records/Reify+Grouped table records/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.213Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.241Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.283Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.301Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.395Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:27.459Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:54.687Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Read+Write records input/Write/WriteImpl/GroupByKey/GroupByWindow+Write records input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:54.791Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:54.919Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:54.956Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:54.987Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:55.044Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:55.067Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:55.119Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:00:55.191Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:15.788Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_3840625137528757271\". You can check its status with the bq tool: \"bq show -j --project_id=probable-pager-266720 dataflow_job_3840625137528757271\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.234Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.327Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.405Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.490Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.556Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:18.623Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:21.232Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:36.628Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_3840625137528757271\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:37.630Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Read+Grouped table records/GroupByWindow+Dedup table records+Write results/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write results/Write/WriteImpl/Pair+Write results/Write/WriteImpl/WindowInto(WindowIntoFn)+Write results/Write/WriteImpl/GroupByKey/Reify+Write results/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:37.710Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:37.763Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:37.850Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.193Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Read+Write results/Write/WriteImpl/GroupByKey/GroupByWindow+Write results/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.270Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.335Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.363Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.390Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.424Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.454Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.499Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:43.569Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:45.916Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:46.009Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:46.078Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:46.162Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:46.230Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:46.297Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:48.975Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:49.120Z: JOB_MESSAGE_DEBUG: Executing success step success37\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:49.555Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:49.602Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:01:49.640Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:01.363Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:01.408Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:01.447Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_14_53_59-3013522702870427948 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run primary_outcomes_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  217421\n",
       "1  217421"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.primary_outcomes_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.primary_outcomes_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.primary_outcomes_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.primary_outcomes_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsible Parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'responsible_parties'> referenced by query SELECT nct_number, name_title, organization, type, investigator_affiliation, investigator_full_name, investigator_title, serialid FROM nih_modeled.responsible_parties limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_c0e945f867e5429db764761d0cb50bad does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.responsible_parties_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'name_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'organization'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_affiliation'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_full_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782625465\n",
      " etag: 'h1pTe+R765mXCMkTtrwsBA=='\n",
      " id: 'probable-pager-266720:nih_modeled.responsible_parties_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782625499\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'name_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'organization'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'type'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_affiliation'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_full_name'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'investigator_title'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/responsible_parties_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'responsible_parties_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run responsible_parties_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpgbm3lh6e', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpgbm3lh6e', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://ncorona-lyme-ncl/staging/responsible-parties-df.1583791391.793926/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T22:03:17.505576Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-09_15_03_16-3526239740320010587'\n",
      " location: 'us-central1'\n",
      " name: 'responsible-parties-df'\n",
      " projectId: 'probable-pager-266720'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T22:03:17.505576Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-09_15_03_16-3526239740320010587]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-09_15_03_16-3526239740320010587?project=probable-pager-266720\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_15_03_16-3526239740320010587 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:16.592Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-09_15_03_16-3526239740320010587. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:16.592Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-09_15_03_16-3526239740320010587.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:19.869Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:20.427Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:20.874Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:20.914Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write results/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:20.947Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Grouped table records: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:20.984Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write records input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.026Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.136Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.273Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.521Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.556Z: JOB_MESSAGE_DETAILED: Fusing consumer Extract table records into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.591Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.612Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WriteBundles/WriteBundles into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.647Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Dedup table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.679Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Pair into Write records input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.709Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write records input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.749Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Reify into Write records input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.782Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/Write into Write records input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.818Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/GroupByKey/GroupByWindow into Write records input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.853Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/Extract into Write records input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.897Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Reify into Extract table records\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.931Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/Write into Grouped table records/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.954Z: JOB_MESSAGE_DETAILED: Fusing consumer Grouped table records/GroupByWindow into Grouped table records/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:21.991Z: JOB_MESSAGE_DETAILED: Fusing consumer Dedup table records into Grouped table records/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.038Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Pair into Write results/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.064Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/WindowInto(WindowIntoFn) into Write results/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.101Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Reify into Write results/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.131Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/Write into Write results/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.167Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/GroupByKey/GroupByWindow into Write results/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.206Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/Extract into Write results/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.243Z: JOB_MESSAGE_DETAILED: Fusing consumer Write records input/Write/WriteImpl/InitializeWrite into Write records input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.282Z: JOB_MESSAGE_DETAILED: Fusing consumer Write results/Write/WriteImpl/InitializeWrite into Write results/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.319Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.351Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.387Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.418Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.626Z: JOB_MESSAGE_DEBUG: Executing wait step start39\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.700Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/DoOnce/Read+Write results/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.732Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/DoOnce/Read+Write records input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.748Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.765Z: JOB_MESSAGE_BASIC: Executing operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.794Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.794Z: JOB_MESSAGE_BASIC: Executing operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.827Z: JOB_MESSAGE_BASIC: Executing operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.864Z: JOB_MESSAGE_BASIC: Finished operation Grouped table records/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.866Z: JOB_MESSAGE_BASIC: Finished operation Write results/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-09_15_03_16-3526239740320010587 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.882Z: JOB_MESSAGE_BASIC: Finished operation Write records input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.920Z: JOB_MESSAGE_DEBUG: Value \"Grouped table records/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.949Z: JOB_MESSAGE_DEBUG: Value \"Write results/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:22.981Z: JOB_MESSAGE_DEBUG: Value \"Write records input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T22:03:47.148Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n"
     ]
    }
   ],
   "source": [
    "%run responsible_parties_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>214643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  214643\n",
       "1  214643"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.responsible_parties_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.responsible_parties_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.responsible_parties_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.responsible_parties_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'secondary_outcomes'> referenced by query SELECT nct_number, measure, time_frame, safety_issue, description, serialid FROM nih_modeled.secondary_outcomes limit 10\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset probable-pager-266720:temp_dataset_71a83ce0db6248a2ae47c821b593eaa0 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table probable-pager-266720.nih_modeled.secondary_outcomes_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583782781408\n",
      " etag: 'R+bgOxzpKvnzaxUuNgw/9w=='\n",
      " id: 'probable-pager-266720:nih_modeled.secondary_outcomes_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583782781441\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'nct_number'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'measure'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'time_frame'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'safety_issue'\n",
      " type: 'BOOLEAN'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'description'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'serialid'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/probable-pager-266720/datasets/nih_modeled/tables/secondary_outcomes_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'nih_modeled'\n",
      " projectId: 'probable-pager-266720'\n",
      " tableId: 'secondary_outcomes_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "%run secondary_outcomes_beam.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run secondary_outcomes_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primary, Duplicate, and Foreign Key Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>217421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count\n",
       "0  217421\n",
       "1  217421"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select count(*) as count from nih_modeled.secondary_outcomes_Beam_DF\n",
    "union all \n",
    "select count(distinct nct_number) as count from nih_modeled.secondary_outcomes_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_number</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nct_number, count]\n",
       "Index: []"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "select nct_number, count(nct_number) as count\n",
    "from nih_modeled.secondary_outcomes_Beam_DF\n",
    "group by nct_number\n",
    "having count(nct_number) > 1 limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count\n",
       "0      0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) as count FROM nih_modeled.secondary_outcomes_Beam_DF tab LEFT JOIN nih_modeled.clinical_studies_main csm ON tab.nct_number = csm.nct_number WHERE csm.nct_number IS NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
